<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning</title>
  
  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css">
    
  
  <!-- Google fonts -->
  <link href="http://fonts.googleapis.com/css?family=Roboto:400,300" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="style.css">
  
  <style>
    .normal {
      font-size: 20px;
    }
  </style>
  <script async="" src="//www.google-analytics.com/analytics.js"></script><script>
  function page_loaded() {
  }
  </script>
  <style>hcfy-result.__hcfy__result__loaded__.__hcfy__result__both__{border: 1px dotted}</style></head>
  
  <body onload="page_loaded()">
  
  <div class="container sec" id="header">
  <a href="https://hci.stanford.edu/">
    <img src="https://pbs.twimg.com/profile_images/2082407340/hci-icon_400x400.png" style="height:75px; float: left; margin-left: 20px;">
  </a>
  <a href="https://svl.stanford.edu/">
    <img src="http://cs.stanford.edu/people/ranjaykrishna/images/svl_logo.png" style="height:75px; float: right; margin-right: 20px;">
  </a>
  <h1 style="font-size:3em">Action Genome Question Answering</h1>
  <h2 style="font-size:1.5em; text-align: center">A Benchmark for Compositional Spatio-Temporal Reasoning</h2>
  <div style="clear:both;"></div>
  </div>
  
  
  
  <div class="system_photo"><img src="system.jpg" style="width:65%; display: block; margin-left: auto; margin-right: auto;"></div>
  <br><br>
  <div class="sechighlight">
  
  <div class="container sec">
  <h2>Action Genome Question Answering (AGQA)</h2>
  
  <div id="coursedesc">
      Visual events are a composition of temporal actions involving actors spatially interacting with objects. 
      When developing computer vision models that can reason about compositional spatio-temporal events, we need benchmarks 
      that can analyze progress and uncover shortcomings. Existing video question answering benchmarks are useful, but they 
      often conflate multiple sources of error into one accuracy metric and have strong biases that models can exploit, making 
      it difficult to pinpoint model weaknesses. We present Action Genome Question Answering (AGQA), a new benchmark for compositional 
      spatio-temporal reasoning. AGQA contains 192M unbalanced question answer pairs for 9.6K videos. We also provide a balanced 
      subset of 3.9M question answer pairs, 3 orders of magnitude larger than existing benchmarks, that minimizes bias by balancing 
      the answer distributions and types of question structures. Although human evaluators marked 86.02% of our question-answer 
      pairs as correct, the best model achieves only 47.74% accuracy. In addition, AGQA introduces multiple training/test splits to 
      test for various reasoning abilities, including generalization to novel compositions, to indirect references, and to more 
      compositional steps.  Using AGQA, we evaluate modern visual reasoning systems, demonstrating that the best models barely perform 
      better than non-visual baselines exploiting linguistic biases and that none of the existing models generalize to novel compositions 
      unseen during training.
  </div>
  <h2>AGQA 2.0</h2>
  <div id="coursedesc">
    We additionally release AGQA 2.0, which incorporates several updates, 
    most notably employs a stricter balancing procedure to more effectively mitigate language bias. 
    This benchmark contains 96.85M question answer pairs and a balanced subset of 2.27M question answer pairs. 
    With the updated balancing procedure, no language-only model performs with more than 51% accuracy on questions with only two answers. 
  </div>
  
  <div class="container sec" style="display:block; margin-left: auto; margin-right: auto; text-align: center">
  
  
  <br><br>
  <div class="instructor">
    <a href="https://madeleinegrunde.github.io/">
    <div class="instructorphoto"><img src="madeleine.jpg"></div> <!-- Changed -->
    <div>Madeleine Grunde-McLaughlin</div>
    </a>
  </div>
  <div class="instructor">
    <a href="https://ranjaykrishna.com/">
    <div class="instructorphoto"><img src="http://cs.stanford.edu/people/ranjaykrishna/images/ranjay.png"></div>
    <div>Ranjay Krishna</div>
    </a>
  </div>
  <div class="instructor">
    <a href="https://graphics.stanford.edu/~maneesh/">
    <div class="instructorphoto"><img src="http://graphics.stanford.edu/~maneesh/images/agrawala-macarthur3-head-square.jpg"></div>
    <div>Maneesh Agrawala</div>
    </a>
  </div>
  </div>
  </div>
  </div>
  
  
  
  <div class="container sec" style="display:block; margin-left: auto; margin-right: auto;">
  
  <h3>AGQA paper</h3>
  <a href="https://arxiv.org/pdf/2103.16002.pdf">
  <img src="AGQA_paper.jpg" style="width:100%;border: 0px solid #AAA;">
  </a>
  <h3>AGQA 2.0 paper</h3>
  <a href="https://arxiv.org/pdf/2204.06105.pdf">
  <img src="AGQA_2.0_paper.jpg" style="width:100%;border: 0px solid #AAA;">
  </a>
  
  
  </div>
  
  
  <div class="sechighlight">
  <div class="container sec">
  <iframe width="840" height="472" style="display:block; margin-left: auto; margin-right: auto;" src="https://www.youtube.com/embed/6Rw1QF9Hono">
  </iframe>
  </div>
  </div>
  
  <div class="container sec">
  
  <h2>Bibtex</h2>
  <pre style="font-size:12px;">@inproceedings{GrundeMcLaughlin2021AGQA,
  title={AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning},
  author={Grunde-McLaughlin, Madeleine and Krishna, Ranjay and Agrawala, Maneesh},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
  }
  </pre>
  </div>
  <div class="sechighlight">
  
  <div class="container sec" style="font-size:18px">
  
  
    <div class="col-md-6">
      <h1 style="text-align: left">Download benchmark data</h1>
      <div id="coursedesc">
      <h3>AGQA 2.0 benchmark</h3>
      <p></p><i style="font-size:12px;">[Updated April 12 2022] AGQA 2.0 contains changes as detailed <a href="https://arxiv.org/pdf/2204.06105.pdf">here</a>. We recommend this version.</i><p></p>
      <p>Download <a href="https://agqa-decomp.cs.washington.edu/data/agqa2/AGQA_balanced.zip">Balanced AGQA</a> with 2.27M questions (2.2G).</p><p>
      </p><p>Download <a href="https://agqa-decomp.cs.washington.edu/data/agqa2/AGQA_unbalanced.zip">Unbalanced AGQA</a> with 96.8M questions (105G).</p><p>
      </p><p>Download <a href="https://agqa-decomp.cs.washington.edu/data/agqa2/AGQA_unbalanced_subset.zip">Small unbalanced AGQA</a> with 3.0M questions (3.5G).</p><p>
      </p><p>Download <a href="https://agqa-decomp.cs.washington.edu/data/agqa2/csvs.zip">CSV formatted questions</a> for evaluation (1.3G).</p><p>
      </p></div>
      
      <div id="coursedesc" style="padding-top: .2em">
      <h3>AGQA benchmark with programs &amp; scene graphs</h3>
      <p></p><i style="font-size:12px;">[Updated August 6 2021] This data additionally contains the programs and scene graphs. Training and testing distributions have remained consistent.</i><p></p>
      <p>Download <a href="https://drive.google.com/uc?export=download&amp;id=1cpw7L59VYD1YoycaQugWAAqH_nRlliDl">Balanced AGQA</a> with 3.6M questions (3.8G).</p><p>
      </p><p>Download <a href="https://drive.google.com/uc?export=download&amp;id=1XHBH68QLO7azpf5YFcjLcC-3NY28G1Mb">Unbalanced AGQA</a> with 196M questions (213G).</p><p>
      </p><p>Download <a href="https://drive.google.com/uc?export=download&amp;id=1kBmr1a6ypQrhaafsRAZ1nW0B8i6E1sHj">Small unbalanced AGQA</a> with 3.0M questions (3.3G).</p><p>
      </p></div>
      <div id="coursedesc" style="padding-top: .2em">
      <h3>AGQA benchmark</h3>
      <p></p><i style="font-size:12px;">This version of the benchmark is showcased in the CVPR 2021 paper.</i><p></p>
      <p>Download <a href="https://downloads.cs.stanford.edu/vision/agqa/AGQA_balanced.zip">Balanced AGQA</a> with 3.9M questions (1.4G).</p><p>
      </p><p>Download <a href="https://downloads.cs.stanford.edu/vision/agqa/AGQA_unbalanced.zip">Unbalanced AGQA</a> with 192M questions (71G).</p><p>
      </p><p>Download <a href="https://downloads.cs.stanford.edu/vision/agqa/AGQA_unbalanced_subset.zip">Small unbalanced AGQA</a> with 3.0M questions (1.1G).</p><p>
      </p></div>
    <p></p></div>
    <div class="col-md-6">
      <h1 style="text-align: left">Download supporting data</h1>
      <h3>Paper</h3>
      Access our <a href="https://arxiv.org/pdf/2103.16002.pdf">paper</a> here.
      <h3>README and supporting data</h3>
      <p>Download <a href="https://drive.google.com/drive/folders/1OMqA90VXY3BQorKFK5xWLSEEkqX31ui-?usp=sharing">README and supporting data</a>.</p><p>
      </p><h3>Scene graphs</h3>
      <p>Download <a href="https://drive.google.com/uc?export=download&amp;id=1CXU0tWpv-1kkkwkNzpU-BwQoAazPR1kR">Scene Graphs</a> of 9,601 videos (1.4G).</p><p>
      </p><h3>Code</h3>
      <p>Code is available <a href="https://github.com/madeleinegrunde/AGQA_baselines_code">here</a>.</p><p>
  
    </p></div>
  </div>
  
  
  
  <!--<div class="row">
  </div>
  
  <div class="container sec">
  <div class="row">
    <div class="col-md-12">
    </div>
  </div>
  </div>-->
  
  
  <div> <!--class="sechighlight">-->
  <div class="container sec" id="footer">
    This work was partially supported by the CRA DREU program, the Stanford HAI Institute, and the Brown Institute.
  </div>
  </div>
  
  <!-- jQuery and Boostrap -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
  </div></body><div style="all: initial;"><div id="__hcfy__" style="all: initial;"></div></div></html>